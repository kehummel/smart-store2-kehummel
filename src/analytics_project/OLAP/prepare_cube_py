"""Module 6: OLAP and cubing script.

The basis of this script was copied from Denise Cases's file: https://github.com/denisecase/smart-sales-example/blob/main/src/analytics_project/olap/cubing.py

DIMENSION TABLES

Product Table
    product name
    product ID - Primary Key

Sales Table
    product ID - Foreign Key
    sales amount
    sale date
    city name

OUTPUT TABLE
    product ID
    product name
    sales amount
    sales date
    city name
    sum and average of sales per product, per city, per month

"""
import pathlib
import sqlite3

import pandas as pd

from analytics_project.utils_logger import logger

# Global constants for paths and key directories

THIS_DIR: pathlib.Path = pathlib.Path(__file__).resolve().parent
DW_DIR: pathlib.Path = THIS_DIR  # src/analytics_project/olap/
PACKAGE_DIR: pathlib.Path = DW_DIR.parent  # src/analytics_project/
SRC_DIR: pathlib.Path = PACKAGE_DIR.parent  # src/
PROJECT_ROOT_DIR: pathlib.Path = SRC_DIR.parent  # smart-store2-kehummel/

# Data directories
DATA_DIR: pathlib.Path = SRC_DIR / "data"
WAREHOUSE_DIR: pathlib.Path = DATA_DIR / "warehouse"

# Warehouse database location (SQLite)
DB_PATH: pathlib.Path = WAREHOUSE_DIR / "smart_sales.db"

# OLAP output directory
OLAP_OUTPUT_DIR: pathlib.Path = DATA_DIR / "olap_cubing_outputs"

# Recommended - log paths and key directories for debugging

logger.info(f"THIS_DIR:            {THIS_DIR}")
logger.info(f"DW_DIR:              {DW_DIR}")
logger.info(f"PACKAGE_DIR:         {PACKAGE_DIR}")
logger.info(f"SRC_DIR:             {SRC_DIR}")
logger.info(f"PROJECT_ROOT_DIR:    {PROJECT_ROOT_DIR}")

logger.info(f"DATA_DIR:            {DATA_DIR}")
logger.info(f"WAREHOUSE_DIR:       {WAREHOUSE_DIR}")
logger.info(f"DB_PATH:             {DB_PATH}")
logger.info(f"OLAP_OUTPUT_DIR:     {OLAP_OUTPUT_DIR}")

# Create output directory if it does not exist
OLAP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def ingest_sales_data_from_dw() -> pd.DataFrame:
    """Ingest sales data from SQLite data warehouse."""
    try:
        conn = sqlite3.connect(DB_PATH)
        sales_df = pd.read_sql_query("SELECT sale_date, product_id, sales_amount, city FROM sale", conn)
        conn.close()
        logger.info("Sales data successfully loaded from SQLite data warehouse.")
        return sales_df
    except Exception as e:
        logger.error(f"Error loading sale table data from data warehouse: {e}")
        raise

def ingest_product_data_from_dw() -> pd.DataFrame:
    """Ingest sales data from SQLite data warehouse."""
    try:
        conn = sqlite3.connect(DB_PATH)
        product_df = pd.read_sql_query("SELECT product_id, product_name FROM product", conn)
        conn.close()
        logger.info("Sales data successfully loaded from SQLite data warehouse.")
        return product_df
    except Exception as e:
        logger.error(f"Error loading sale table data from data warehouse: {e}")
        raise

def create_olap_cube(sales_df: pd.DataFrame, dimensions: list, metrics: dict) -> pd.DataFrame:
    """Create an OLAP cube by aggregating data across multiple dimensions.

    Args:
        sales_df (pd.DataFrame): The sales data.
        dimensions (list): List of column names to group by.
        metrics (dict): Dictionary of aggregation functions for metrics.

    Returns:
        pd.DataFrame: The multidimensional OLAP cube.
    """
    try:
        # Group by the specified dimensions and aggregate metrics
        # When we use the groupby() method in Pandas,
        # it creates a hierarchical index (also known as a MultiIndex) for the grouped data.
        # This structure reflects the grouping levels but can make the resulting DataFrame
        # harder to interact with, especially for tasks like slicing, querying, or merging data.
        # Converting the hierarchical index into a flat table by calling reset_index()
        # simplifies our operations.

        # NOTE: Pandas generates hierarchical column names when we specify
        # multiple aggregation functions for a single column.
        # If we specify only one aggregation function per column,
        # the resulting column names will not include the suffix.

        # Group by the specified dimensions
        grouped = sales_df.groupby(dimensions)

        # Perform the aggregations
        cube = grouped.agg(metrics).reset_index()

        # Add a list of sale IDs for traceability
        cube["product_ids"] = grouped["product_id"].apply(list).reset_index(drop=True)

        # Generate explicit column names
        explicit_columns = generate_column_names(dimensions, metrics)
        explicit_columns.append("product_ids")  # Include the traceability column
        cube.columns = explicit_columns

        logger.info(f"OLAP cube created with dimensions: {dimensions}")
        return cube
    except Exception as e:
        logger.error(f"Error creating OLAP cube: {e}")
        raise


def generate_column_names(dimensions: list, metrics: dict) -> list:
    """Generate explicit column names for OLAP cube, ensuring no trailing underscores.

    Args:
        dimensions (list): List of dimension columns.
        metrics (dict): Dictionary of metrics with aggregation functions.

    Returns:
        list: Explicit column names.
    """
    # Start with dimensions
    column_names = dimensions.copy()

    # Add metrics with their aggregation suffixes
    for column, agg_funcs in metrics.items():
        if isinstance(agg_funcs, list):
            for func in agg_funcs:
                column_names.append(f"{column}_{func}")
        else:
            column_names.append(f"{column}_{agg_funcs}")

    # Remove trailing underscores from all column names
    column_names = [col.rstrip("_") for col in column_names]
    logger.info(f"Generated column names for OLAP cube: {column_names}")

    return column_names


def write_cube_to_csv(cube: pd.DataFrame, filename: str) -> None:
    """Write the OLAP cube to a CSV file."""
    try:
        output_path = OLAP_OUTPUT_DIR.joinpath(filename)
        cube.to_csv(output_path, index=False)
        logger.info(f"OLAP cube saved to {output_path}.")
    except Exception as e:
        logger.error(f"Error saving OLAP cube to CSV file: {e}")
        raise


def main():
    """Execute OLAP cubing process."""
    logger.info("Starting OLAP Cubing process...")

    # DEBUG: Detailed path checking
    logger.info(f"Database file exists: {DB_PATH.exists()}")
    logger.info(f"Database is a file: {DB_PATH.is_file()}")
    logger.info(f"Parent directory exists: {DB_PATH.parent.exists()}")

    # Check if we can read the file
    import os
    try:
        logger.info(f"Can read database: {os.access(DB_PATH, os.R_OK)}")
        logger.info(f"Can write to database: {os.access(DB_PATH, os.W_OK)}")
        logger.info(f"File size: {DB_PATH.stat().st_size} bytes")
    except Exception as e:
        logger.error(f"Error checking file access: {e}")

    # Step 1a: Ingest sales data
    sales_df = ingest_sales_data_from_dw()
    if sales_df.empty:
        logger.warning(
            "WARNING: The sales table is empty. "
            "The OLAP cube will contain only column headers. "
            "Fix: Prepare raw data and run the ETL step to load the data warehouse."
        )

    # Step 1b: Ingest products data
    product_df = ingest_product_data_from_dw()
    if product_df.empty:
        logger.warning(
            "WARNING: The sales table is empty. "
            "The OLAP cube will contain only column headers. "
            "Fix: Prepare raw data and run the ETL step to load the data warehouse."
        )

    # Step 2.5: Merge sales and product data
    sales_df: pd.DataFrame = sales_df.merge(product_df, on="product_id", how="left")
    logger.info("Sales and product data merged successfully.")

    # Step 2: Add additional columns for time-based dimensions
    sales_df["sale_date"] = pd.to_datetime(sales_df["sale_date"])
    sales_df["Month"] = sales_df["sale_date"].dt.month

    # Step 3: Define dimensions and metrics for the cube
    dimensions = ["Month", "product_id", "product_name", "city"]
    metrics = {"sales_amount": ["sum", "mean"], "product_id": "count"}

    # Step 4: Create the cube
    olap_cube = create_olap_cube(sales_df, dimensions, metrics)

    # Step 5: Save the cube to a CSV file
    write_cube_to_csv(olap_cube, "multidimensional_olap_cube.csv")

    logger.info("OLAP Cubing process completed successfully.")
    logger.info(f"Please see outputs in {OLAP_OUTPUT_DIR}")


if __name__ == "__main__":
    main()
